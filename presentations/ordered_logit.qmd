---
title: "Multicategory Models"
subtitle: "The Ordered Logit and Ordered Probit"
date: "`r Sys.Date()`"
author: "Christopher Weber"
organization: "University of Arizona"
email: "chrisweber@arizona.edu"
format: 
  revealjs:
    theme: simple   
    slide-number: true
    toc: false
    toc-depth: 2
    code-fold: true
    code-summary: "Show/Hide Code"
    transition: fade
    center: true
    width: 1200
    height: 800
    slide-level: 2
    navigation-mode: linear
    self-contained: false
    css: styles.css
title-block-banner: "#378DBD"  
---

## Overview {.smaller}

- These notes track the assigned reading in Long (1997) on model fit, interpetation, and the ordered regression model.

- These notes also follow McElreath (2016) in *Statistical Rethinking*, Chapters 10 and 11.

- The problem with the categorical models we've explored is that they are not directly interpretable, due to the non-linearity of the effect of $x$ on $y$.

-   Recall the non-linearity and non-additive of the binomial model.

$${{\partial F(x)}\over{\partial x}}=b \times f(x)$$

## Non Linearity {.smaller}

-   For the logit:

$${{\partial F_{logit}(x)}\over{\partial x}}=b \times {{exp(a+bx)}\over{1+exp(a+bx)^2}}$$

-   *For probit, just replace f(x) with the normal PDF*

## Non Additivity  {.smaller}

-   It is even more challenging to interpret the results when there are other covariates, since:

$${{\partial F_{logit}(x)}\over{\partial x_k}}=b_k \times {{exp(a+\sum b_j x_j)}\over{1+exp(a+\sum b_j x_j)^2}}$$

-   The model is linear with respect to the log odds, but not probabilities.

-   **We cannot directly compare the logit to probit coefficients.**

-   Every coefficient in the model represents the expected log change in the odds for a unit change in $x$ (logit).

-   Unlike an OLS model, the coefficients in the logit (or probit) are not directly interpretable.

-   **The model is non-additive and non-linear.**

-  **How do we interpret the coefficients?**

## Interpretation {.smaller}

-   **Standardize**. Divide the coefficient by the standard error. Now the interpretation is, "for a one unit change in x we anticipate a b standard deviation change in $y_{latent}$"

-   **Odds Ratio**. Exponentiate the coefficient and interpret the odds ratio, $exp(b)$. This only applies to the logit model.

-   **Predictive Effects**. Calculate the predicted probability of $y=1$ for a change in $x$ from some value to another value, holding all other variables constant, at **a fixed value**.

-   **Marginal Effects**. Calculate the predicted probability of $y=1$ for a change in $x$ from some value to another specified value, holding all other variables constant.

- **Tools**. `glm::predict`, `ggplot2`, `MASS`

## Model Fit and Maximum Likelihood {.smaller}

-   Model fit is an important, though difficult, topic when we are dealing with non-linear models.

-   We may derive scalar measures of model fit from the linear model, like, $R^2$.

-   It's hard to find a comparably reliable statistic for non-linear models.

-   We never observe $y_{latent}$ directly.

-   What is **"percent variance explained"** on an unobserved scale?


## Counts correctly predicted {.smaller}

-   A matrix which is the predicted value of $y_{obs}$ and the actual value of $y$.

-   In this 2x2 matrix, the 1,1 and 0,0 entries represent accurate predictions.

-   The off-diagonals are inaccurate predictions.

-  A **confusion matrix**.

-   Generate predictions for when the predicted latent variable is positive ($y=1$) or negative ($y=0$).

-   Then, calculate the **percent correctly predicted**.


## Correcting for Chance {.smaller}

-   **Percent Correctly Predicted** is a good starting point.

-   $pr(Y=1) = 0.55$ 

-   *versus*:  $pr(Y=1) = 0.91$ 

-   We may be more confident in the latter.

-   Weight each prediction by its constituent probability.

-   We are accounting for our uncertainty. Typically, this number is somewhat lower than PCP.

-   If we convert these to probabilities by using the inverse normal or logit ($\texttt{pnorm}$ or $\texttt{logit}$), then define the $ePCP$, expected correctly predicted as:

$$ePCP={1\over n}({\sum_{y=1} P_i+\sum_{y=0}(1-P_i)})$$


## Reduction in Error {.smaller}

-   **Assume we estimate two models**
-   A naive model and a model with the expected predictor, **Model 1**.
-   The naive model predicts the outcome based on the modal category, **Model 2**
-   *If 51% voted for the Republican, the model would predict a Republican vote with probability of 0.51. We should never really get less than 0.51 -- if we do, then the naive model would be a superior model.*
-   *If 78% voted for the Liberal Party, the model would predict a Liberal vote with probability of 0.78. We should never really get less than 0.78 -- if we do, then the naive model would be a superior model. Likewise, if our complicated, proposed model predicts a 0.80 probability, the new model doesn't seem to reduce error.*

## Chance {.smaller}

```{r, echo = TRUE}
## Load required packages
library(dplyr)
library(MASS)
library(pscl)
load("~/Dropbox/github_repos/teaching/POL683_Fall24/advancedRegression/vignettes/dataActive.rda")
dat = dataActive %>% 
  mutate(
    pid = recode(pid3, "Democrat" = 1, "Independent" = 2, "Republican" = 3, "Other" = 2, "Not sure" = 2),
    protest = ifelse(violent > 3, 1, 0)
  ) 
# Esitmate logit
my_model =  glm(protest ~ as.factor(pid),
       family=binomial(link="logit"), data = dat)
# Produce confusion matrix.
hitmiss(my_model)
```

## Interpretation {.smaller}

-   If we were to just estimate $\theta$, that value would be the same as $\texttt{plogis(a)}$ from a regression model with no predictors.

-   The naive model is one that just assumes a single underlying $\theta$, instead of $\theta$ being some linear composite of predictors. Then, we may construct a comparison.

$$PRE={{PCP-PMC}\over {1-PMC}}$$

-   PRE is simply the proportional reduction in error.

## The Likelihood Ratio Test  {.smaller}

* How do we test whether multiple predictors jointly improve model fit?

**Example:** Test $H_0: \beta_{1}=\beta_{2}=0$ (both slopes are zero).

* Set up two nested models

- **Null Model ($M_0$):** $y = \beta_0$ (intercept only)
- **Full Model ($M_1$):** $y = \beta_0 + \beta_1x_1 + \beta_2x_2$ (includes predictors)

## The Likelihood Ratio Test  {.smaller}

- **Null Model ($M_0$):** $y = \beta_0$ (intercept only)
- **Full Model ($M_1$):** $y = \beta_0 + \beta_1x_1 + \beta_2x_2$ (includes predictors)

**The Likelihood Ratio Test Statistic:**

$$
G^2 = 2(\log L_{M_1} - \log L_{M_0}) = 2\log L_{M_1} - 2\log L_{M_0}
$$

where:

- $\log L_{M_1}$ = log-likelihood of the full model.
- $\log L_{M_0}$ = log-likelihood of the null model.

**Interpretation:**

- Large $G^2$ values suggest the full model fits significantly better.
- Under $H_0$, $G^2 \sim \chi^2$ with degrees of freedom = difference in number of parameters.
- Similar to the F-test for linear regression, but uses likelihood instead of sum of squares.

**Key insight:** This tests whether adding $x_1$ and $x_2$ jointly improves the model fit beyond just having an intercept.


## An Example {.smaller}

```{r, echo = TRUE}
library(lmtest)
load("~/Dropbox/github_repos/teaching/POL683_Fall24/advancedRegression/vignettes/dataActive.rda")
dat = dataActive %>% 
  mutate(
    pid = recode(pid3, "Democrat" = 1, "Independent" = 2, "Republican" = 3, "Other" = 2, "Not sure" = 2),
    protest = ifelse(violent > 4, 1, 0)
  ) 
a =  glm(protest ~ 1,
       family=binomial(link="logit"), data = dat)

b =  glm(protest ~ as.factor(pid),
       family=binomial(link="logit"), data = dat)

lrtest(a, b)
```

## $G^2$   {.smaller}

-   The $G^2$ statistic is distributed $\chi^2$ with $df=$number of constraints (here 2). Clearly, we can reject the null of no influence, see Long (1997, page 94).

-   We could flip things, and instead of comparing our model to one with no predictors, we could compare our model to one with predictors equal to the number of data points.


$$G^2=2 loglik_{Full}-2 loglik_{M_1}$$

$$Deviance=-2 loglik_{M_1}$$ 

*  This is the **deviance**. It is just two times the log likelihood.

## Deviance  {.smaller}

**Deviance** = measure of lack of fit (lower is better).

**Null Deviance:**
$$D_{null} = 2\log L_{saturated} - 2\log L_{null}$$


* Compares saturated model vs. **intercept-only**. Shows total variability to explain.

**Residual Deviance:**
$$D_{residual} = 2\log L_{saturated} - 2\log L_{model}$$

* Compares saturated model vs. **proposed model**. Unexplained variability.

## Deviance  {.smaller}

$$D_{null} - D_{residual} = G^2 = 2(\log L_{model} - \log L_{null})$$

- Difference = likelihood ratio test statistic.
- Tests if predictors improve fit over null.
- Analogous to $R^2$ (proportion of deviance explained).

## The Wald Test {.smaller}

-   The Wald Test is asymptotically equivalent to the LR test.

-  Test hypotheses about parameters using only the **unrestricted model**.

**The Logic:**

1. Estimate your model: $\hat{\boldsymbol{\beta}}$ and $Var(\widehat{\boldsymbol{\beta}})$.
2. Measure how far estimates are from null hypothesis.
3. Scale by standard errors to get test statistic.


## Single Parameter  {.smaller}

**Test $H_0: \beta_1 = 0$**

Wald statistic:

$$W = \frac{(\hat{\beta}_1 - 0)^2}{Var(\hat{\beta}_1)} = \frac{\hat{\beta}_1^2}{SE(\hat{\beta}_1)^2}$$

- Notice the similarity to a z-test

$$z = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}, \quad W = z^2$$

- Under $H_0$: $z \sim N(0,1)$ 
- $W \sim \chi^2_1$

**This is just the standard z-test**

## Multiple Parameters {.smaller}

- We can test more complex hypotheses involving multiple parameters.

- The test is based on the distance between the estimated parameters and the null hypothesis values, scaled by their covariance matrix.

**$H_0: Q\boldsymbol{\beta} = \boldsymbol{r}$**

$$W = (Q\hat{\boldsymbol{\beta}} - \boldsymbol{r})^T [Q  Var(\hat{\boldsymbol{\beta}})  Q^T]^{-1} (Q\hat{\boldsymbol{\beta}} - \boldsymbol{r})$$

- $\widehat{\boldsymbol{\beta}}$: vector of estimated coefficients.
- $Q$: matrix selecting which parameters to test.
- $\boldsymbol{r}$: hypothesized values (typically, but not always, 0$).
- $Var(\widehat{\boldsymbol{\beta}})$: variance-covariance matrix.
- Under $H_0$: $W \sim \chi^2_{df}$ where $df$ = number of constraints.




## Deconstructing the Wald Test {.smaller}


$$W=(Qb-r)^T(Qvar(b)Q^T)^{-1}(Qb-r)$$

-   The left and rightmost portions estimate the distance between the actual value of $b$ and 0 -- regardless of the complexity.

-   The freed model relative to the constrained model.

-   Because there is uncertainty around the estimates, this is represented in the middle portion. Again, we multiple by Q because we are only concerned about $b$.

## When to Use {.smaller}

-   Compares **nested** models.

-   The Wald and LR tests are reasonable approaches, but...

-   Their small sample properties are not always well defined.

-   **They should only be used if the $\textit{null}$ model consists of the same data.**

-   These methods can really only be used for nested model. In the case above, $b=0$ is a constraint, so the restricted/constrained model is nested in the unrestricted model.

## Scalar Estimates Fit{.smaller}

-   Scalar estimates of model fit are less intuitive in the logit/probit framework. 

$$R^2={RegSS\over TSS}=1-{RSS/TSS}$$

-   The problem is that in the logit/probit model, we cannot directly compare $Y_{obs}$ to the prediction we make for $Y_{latent}$.

-    *Pseudo-*$R^2$ uses the latent prediction (Efron 1978) 

$$Pseudo-R^2={1-{\sum (y-\hat{y}_{latent})^2}\over {\sum (y-\bar{y})^2)}}$$

## Information Measures {.smaller}

-   The Akaike Information Criterion (AIC) 

$$AIC={{-2loglik(\theta)+2P}\over N}$$

-   Calculate the $-2loglik$ and add 2 $\times$ the number of predictors, where $p=K+1$ (Long 1997, 109).

-   Finally, divide by the number of observations. Notice what happens with this function. As the number of parameters increases, but the log-likelihood stays the same, the AIC will increase.

-   Should prefer a smaller AIC. The statistic penalizes for added parameters that do not improve fit.

## BIC {.smaller}

-   The *Bayesian Information Criterion (BIC)* is based on a comparision -- between a fully saturated model and the proposed model. The BIC is:

$$BIC=D(M)-df \times log(N)$$

-  $D(M)$ is simply the deviance for the model 

-  The degrees of freedom calculation is $N-k-1$, where $k$ is the number of predictors.

## The Ordered Logit  {.smaller}

-   This summary follows your assigned reading in Long (1997) and McElreath, Chapter 11.

-   Only use an ordered parameterization when we have ordered data.

-   Some data can be ordered, even if they are theoretically multidimensional; others should be modeled differently.

-   Examples: PID, Ideology (social and economic dimensions)

-   "How much do you agree or disagree with the following item?" from "1" Strongly Disagree to "5" Strongly Agree.

## Why not OLS?  {.smaller}

-   Ordered, non-interval level data may violate the assumptions of the classical linear regression model.

1)  Non-constant variance.

2)  Predictions may be non-sensical (i.e., we predict things outside of the observed bounds).

3)  If the category distances are theoretically quite different.

## Probit or Logit {.smaller}

-   Whether you choose ordered logit or probit is often just a matter of personal preference. I'll just use the logit for now, but specifying a probit is just a matter of changing the link-function.

-   The ordered logit is also called the proportional odds regression model. 

-   It is a generalization of the binary logit, using the logic of **accumulating probabilities**. 

-  Conceptually just think about it as a number of binary logits, where the cutpoints slice the latent distribution into discrete categories.

- In the ordered logit or probit parameterization, we do not estimate the intercept, $\beta_0$, because it is not uniquely identified from the cutpoints, which serve as intercepts for cumulative logits. 

- Estimating an ordered logit model on binary data reduces to the binary logit model.

- Instead of one cutpoint -- the intercept in logit -- now we estimate $k-1$ cutpoints. 

## Ordered Logit {.smaller}

$$y_{latent} = Xb + e$$
$$
\begin{eqnarray*}
P(y=1) 
  & = & P(\tau_0 \leq y_{latent} < \tau_1) \\
  & = & P(\tau_0 \leq \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} < \tau_1) \\
  & = & P(\tau_0 - \mathbf{X}\boldsymbol{\beta} \leq \boldsymbol{\varepsilon} < \tau_1 - \mathbf{X}\boldsymbol{\beta}) \\
  & = & P(\boldsymbol{\varepsilon} < \tau_1 - \mathbf{X}\boldsymbol{\beta}) - P(\boldsymbol{\varepsilon} < \tau_0 - \mathbf{X}\boldsymbol{\beta}) \\
  & = & F(\tau_1 - \mathbf{X}\boldsymbol{\beta}) - F(\tau_0 - \mathbf{X}\boldsymbol{\beta})
\end{eqnarray*}
$$


## Ordered Logit {.smaller}

- $k = 4$, so **length($\tau$) = 3**

$$
\begin{eqnarray*}
P(y=1) & = & F(\tau_1 - \mathbf{X}\boldsymbol{\beta}) \quad \text{where } \tau_0 = -\infty \\
P(y=2) & = & F(\tau_2 - \mathbf{X}\boldsymbol{\beta}) - F(\tau_1 - \mathbf{X}\boldsymbol{\beta}) \\
P(y=3) & = & F(\tau_3 - \mathbf{X}\boldsymbol{\beta}) - F(\tau_2 - \mathbf{X}\boldsymbol{\beta}) \\
P(y=4) & = & 1 - F(\tau_3 - \mathbf{X}\boldsymbol{\beta}) \quad \text{where } \tau_4 = \infty
\end{eqnarray*}
$$


## The Ordered Logit {.smaller}

-   $y_{latent}$, where $y_{obs} \in (1,2,3,...k)$.

- **Accumulated Comparisons**: With a four category outcome, we can think of three models. Compare **category 1** to **category 2, 3, 4**; then compare **categories 1, 2** to **categories 3, 4**; and finally compare **categories 1, 2, 3** to **category 4**.

- It's useful to envision this as three individual logit models, but where the parameters, $\beta$ are constrained to the the same across the three models. This is called the **parallel regression assumption** or the **parallel lines assumption**.

- This is because it involves cumulative odds.

## The Ordered Logit {.smaller}


$$
\begin{aligned}
\boldsymbol{y}_{observed} &\sim \text{Ordered}(\mathbf{p}) \\
p_j &= P(y_{latent} = j) = F_{logit}(\tau_{j} - \mathbf{X}\boldsymbol{\beta}) - F_{logit}(\tau_{j-1} - \mathbf{X}\boldsymbol{\beta}) \\
\boldsymbol{y}_{latent} &= \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{aligned}
$$

* where $F_{logit}(z) = \frac{1}{1 + e^{-z}}$ is the logistic CDF.
* **p** is a vector of probabilities for each category, summing to 1.
* The **j** element of $p$ is the probability of observing category **j**.
* And the cumulative log odds is a function of variable matrix, **X**, the cutpoints, $\tau$, and the coefficients, $\beta$ and an error term **e**, distributed following the logit distribution. 


## Parallel Lines  {.smaller}
```{r}
library(plotly)
x <- seq(-10, 10, length.out = 100)
logit <- function(x) 1 / (1 + exp(-x))

y1 <- logit(x + 1)
y2 <- logit(x + 3)
y3 <- logit(x + 5)

data <- data.frame(x, y1, y2, y3)

fig <- plot_ly(data, x = ~x) |>
  add_trace(y = ~y1, type = 'scatter', mode = 'lines', name = 'Categories 2,3,4 versus Categories 1') |>
  add_trace(y = ~y2, type = 'scatter', mode = 'lines', name = 'Categories 3,4 versus Categories 1,2') |>
  add_trace(y = ~y3, type = 'scatter', mode = 'lines', name = 'Category 4 versus Categories 1, 2, 3') |>
  layout(title = 'Parallel Lines',
         xaxis = list(title = 'X-axis'),
         yaxis = list(title = 'Y-axis'))

fig
```

- Each line corresponds to a log odds of combining categories into a **cumulative log odds** where the lines are parallel, *or the odds ratios are constant, they are proportional*. The distance between the lines is constant, which means that the effect of $X$ is the same across all cumulative comparisons.


## The Structural Model  {.smaller}

$$y_{latent}=\beta_0 + \beta_1x_i +...\sum^{J}_{j =1} \beta_j x_{ij}+e_i$$

$$y=X\beta+e$$

- There is no intercept term, as these are represented by the cutpoints. But it's useful to think of the cutpoints as intercepts for each of the cumulative logits.


## The Measurement Model  {.smaller}

-   Instead of the variable being 0/1, it is not more than two categories that are ordered. Assume we knew $y_{latent}$ and would like to map that to observing a particular category.

-   Using the same logic from the binary regression model, assume that we observe the category based on the orientation to a series of cutpoints, where

$$y_i=m: \tau_{k-1}\leq y_{latent} < \tau_{k}$$

- In `MASS::polr()` these are `zeta`



## The Likelihood  {.smaller}

$$
\begin{eqnarray*}
pr(y_{i}=k|X_i) & = & F(\tau_k-\alpha-X_i\beta)-F(\tau_{k-1}-\alpha-X_i\beta) \\
\end{eqnarray*}
$$

- The likelihood of the ordered logit or probit model is the joint probability of being in each category, so we need to calculate the **likelihood** ($L(y|\theta)$) as 

$$
Pr(y_{i}=1|X_i)\times pr(y_{i}=2|X_i) \times pr(y_{i}=3|X_i) \times....pr(y_{i}=K|X_i)
$$.

- This is just the joint probability for category membership, for each subject, so

$$
\begin{eqnarray*}
Pr(y_{i}|X_i) & = & \prod_{k=1}^K F(\tau_k-\alpha-X_i\beta)-F(\tau_{k-1}-\alpha-X_i\beta) \\
\end{eqnarray*}
$$

## The Likelihood, Continued   {.smaller}

$$
\begin{eqnarray*}
Pr(y_{i}|X_i) & = & \prod_{k=1}^K F(\tau_k-\alpha-X_i\beta)-F(\tau_{k-1}-\alpha-X_i\beta) \\
\end{eqnarray*}
$$


- This only references the probability space for a single subject. Since the likelihood is $\prod_{i=1}^N p_i$, we need to calculate the joint probability for each subject.

$$
\begin{eqnarray*}
pr(y|X) & = & \prod_{i=1}^N \prod_{k=1}^K F(\tau_k-\alpha-X_i\beta)-F(\tau_{k-1}-\alpha-X_i\beta) \\
L(\beta \tau | y, X)& = & \prod_{i=1}^N \prod_{k=1}^K F(\tau_k-\alpha-X_i\beta)-F(\tau_{k-1}-\alpha-X_i\beta) \\
\end{eqnarray*}
$$ 

## The Log Likelihood {.smaller}

\begin{eqnarray*}
Loglik(\beta \tau | y, X)& = & \sum_{i=1}^N \sum_{k=1}^K log[F(\tau_k-\alpha-X_i\beta)-F(\tau_{k-1}-\alpha-X_i\beta)] \\
\end{eqnarray*}

-   Like the binary case: $x \rightarrow y_{latent} \rightarrow y_{obs}$.

-  The only thing that is different is that instead of a single cutpoint -- at 0 -- we have a series of cutpoints, corresponding to **the number of categories minus 1**.

## The Western States Data (2020) {.smaller}

- Let's estimate an ordered logit model in `R`, from the `MASS` package. Data are collected **pre** or **post** election, and we want to see if support for electoral contestation behavior (here, attending a march) changes in support over this period -- for Trump voters versus Biden voters. This is specified to examine whether support for contestation varies depending upon electoral functions; a **winner-loser effect**

```{r, warning = FALSE, message = FALSE, echo = FALSE}
library(dplyr)
library(tidyr)
# devtools::install_github("crweber9874/regressionEffects")
library(regressionEffects)

download.file("https://raw.githubusercontent.com/crweber9874/advancedRegression/main/data/wss20.rda",
              destfile = "wss20.rda")
## Load into session
load("wss20.rda")

wss20 = wss20 |>
  pivot_wider(
    names_from = contestation,
    values_from = contestation_value
  )

sample_df <- wss20 |>
  mutate(contestation = rowMeans(cbind(attend_march, criticize_election, burn_flag, court, recount), na.rm = 
                                   TRUE), # Construct a continuous scale
        vote_trump = presvote_trump_2020,
        authoritarianism = rowMeans(cbind(auth_1, auth_2, auth_3, auth_4), na.rm = TRUE),
        republican = ifelse(party_identification3 == 3, 1, 0),
        democrat = ifelse(party_identification3 == 1, 1, 0),
        independent = ifelse(party_identification3 == 2, 1, 0),
        libcon = (ideology5 - 1)/4,
        prepost = ifelse(prepost == "post", 1, 0)
)
head(sample_df)
```

## Estimating an Ordered Logit {.smaller}

```{r}
library(MASS)
my_model = polr(as.factor(burn_flag) ~ prepost*vote_trump , 
               data = sample_df)

my_model |> summary()
```

 - There is clearly an interaction effect -- support varies depending on whether the observation was before or after the election **and** whether the respondent voted for Trump or Biden. The sign of the lower order and interaction terms seems to indicate that Trump supporters are more supportive, post-election; Biden voters are more supporting in the pre-election. But how should we interpret this.

##  Predicted Probabilities {.smaller}

```{r, warning = FALSE, message = FALSE, echo = FALSE}
design_matrix <- expand.grid(
   prepost = c(0,1),
   vote_trump = c(0,1)
 )
predictions_burn <- predict_ordinal_probs(
  design_matrix = design_matrix,
  model = my_model,
  n_draws = 1000)
summarize_predictions(predictions_burn) |> print()
```

- The predicted probabilities indicate that Trump voters are more supportive of burning the flag, post-election. Biden voters are less supportive, post-election.

##  Predicted Probabilities {.smaller}


```{r, warning = FALSE, message = FALSE, echo = FALSE}
library(regressionEffects)
plot_burn  <- prepare_prediction_data(sample_df, predictions_burn)
design_matrix <- expand.grid(
   prepost = c(0,1),
   vote_trump = c(0,1)
 )
plotDots(plot_burn$plotting_data |>
          filter(vote_trump == 1), 
          x_var = "category",
          y_var = "mean_prob",
          color_var = "prepost",
          color_levels = c(0, 1),
          color_labels = c("Pre", "Post"),
          color_values = c("grey", "black"),
          title = "Flag Burning",
         y_label =  "Mean Estimate"
)
```

##  Predicted Probabilities {.smaller}

```{r, warning = FALSE, message = FALSE, echo = FALSE}
devtools::load_all()
plotDots(plot_burn$plotting_data |>
          filter(vote_trump == 0), 
          x_var = "category",
          y_var = "mean_prob",
          color_var = "prepost",
          color_levels = c(0, 1),
          color_labels = c("Pre", "Post"),
          color_values = c("grey", "black"),
          title = "Flag Burning",
          y_label =  "Mean Estimate",

)

```
## Changes in Probabilities {.smaller}

```{r, warning = FALSE, message = FALSE, echo = FALSE}
create_combined_dot_plot(
   plot_data = plot_burn,
   predictions = predictions_burn,
   main_title = "Flag Burning",
   moderator = "vote_trump",
   moderator_levels = c(0, 1),
  group_labels = c("Biden Voters", "Trump Voters")
)
```
